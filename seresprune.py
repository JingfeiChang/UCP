# -*- coding: utf-8 -*-
"""
Created on Mon Sep 30 18:05:16 2019

@author: ASUS
"""

import argparse
import numpy as np
import os

import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
from torch.autograd import Variable
from torchvision import datasets, transforms

import models.cifar3 as models
from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig
from ptflops import get_model_complexity_info

# Prune settings 
parser = argparse.ArgumentParser(description='PyTorch Slimming CIFAR prune')
parser.add_argument('--dataset', type=str, default='cifar100',
                    help='training dataset (default: cifar10)')
parser.add_argument('--test-batch-size', type=int, default=10000, metavar='N',
                    help='input batch size for testing (default: 256)')
parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='disables CUDA training')
parser.add_argument('--depth', type=int, default=110,
                    help='depth of the resnet')
parser.add_argument("--reduction", type=int, default=16)
parser.add_argument('--model', default='./results-se_resnet5/model_best.pth.tar', type=str, metavar='PATH',
                    help='path to the model (default: none)')
parser.add_argument('--resume', default='', type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('--save', default='.', type=str, metavar='PATH',
                    help='path to save pruned model (default: none)')
parser.add_argument('--arch', default='se_resnet', type=str, 
                    help='architecture to use')
parser.add_argument('-v', default='A', type=str, 
                    help='version of the model')

args = parser.parse_args()

state = {k: v for k, v in args._get_kwargs()}

args.cuda = not args.no_cuda and torch.cuda.is_available()

if not os.path.exists(args.save):
    os.makedirs(args.save)

model = models.__dict__[args.arch](dataset=args.dataset, depth=args.depth, reduction=args.reduction)

if args.cuda:
    model.cuda()

print(model)



if args.model:
    if os.path.isfile(args.model):
        print("=> loading checkpoint '{}'".format(args.model))
        checkpoint = torch.load(args.model)
        args.start_epoch = checkpoint['epoch']
        best_acc = checkpoint['best_acc']
        model.load_state_dict(checkpoint['state_dict'])
        print("=> loaded checkpoint '{}' (epoch {}) best_acc: {:f}"
              .format(args.model, checkpoint['epoch'], best_acc))
    else:
        print("=> no checkpoint found at '{}'".format(args.resume))

print('Pre-processing Successful!')

# simple test model after Pre-processing prune (simple set BN scales to zeros)
def test(model):
    kwargs = {'num_workers': 0, 'pin_memory': True} if args.cuda else {}
    if args.dataset == 'cifar10':
        test_loader = torch.utils.data.DataLoader(
            datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),
            batch_size=args.test_batch_size, shuffle=False, **kwargs)
    elif args.dataset == 'cifar100':
        test_loader = torch.utils.data.DataLoader(
            datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),
            batch_size=args.test_batch_size, shuffle=False, **kwargs)
    else:
        raise ValueError("No valid dataset is given.")

    correct = 0
    for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        with torch.no_grad():
            data, target = Variable(data), Variable(target)
            output = model(data)
            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
            correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    print('\nTest set: Accuracy: {}/{} ({:.1f}%)\n'.format(
        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))
    return 10000 * correct / float(len(test_loader.dataset))

acc = test(model)
acc=acc.numpy()

num_parameters = sum([param.nelement() for param in model.parameters()])
print("number of parameters: "+str(num_parameters)+"\n")
with open(os.path.join(args.save, "res56prune.txt"), "w") as fp:
    fp.write("Number of parameters: \n"+str(num_parameters)+"\n")
    fp.write("Test accuracy: \n"+str(acc)+"\n")

# 计算模型参数
with torch.cuda.device(0):
  net = model
  flops, params = get_model_complexity_info(net, (3, 32,32), as_strings=True, print_per_layer_stat=True)
  print('Flops:  ' + flops)
  print('Params: ' + params)


#******************************************************************************************************#

# 模型裁剪

imscore = [
[0.3669655919075012, 0.3460102677345276, 0.37293189764022827, 0.5243672728538513, 0.7011353373527527, 0.5340172648429871, 0.3744712173938751, 0.32879284024238586, 0.41971054673194885, 0.29366442561149597, 0.491147518157959, 0.5990291237831116, 0.7127014398574829, 0.4871695935726166, 0.6003080010414124, 0.2792074680328369],
[0.393245130777359, 0.5977595448493958, 0.27906861901283264, 0.2877700626850128, 0.6869537830352783, 0.42653876543045044, 0.4451832175254822, 0.7187926173210144, 0.39690157771110535, 0.2728346884250641, 0.3181522488594055, 0.5938290357589722, 0.3102579414844513, 0.38652434945106506, 0.5310484170913696, 0.5177890658378601],
[0.6628153920173645, 0.5569208264350891, 0.5878300070762634, 0.3570000231266022, 0.36275753378868103, 0.6600136160850525, 0.6561225652694702, 0.4100415110588074, 0.34749507904052734, 0.3996739089488983, 0.4060962498188019, 0.5577099919319153, 0.40276119112968445, 0.7074757218360901, 0.3376593589782715, 0.582086443901062],
[0.4563450813293457, 0.5629525184631348, 0.35519805550575256, 0.5909548401832581, 0.6699658632278442, 0.2772287130355835, 0.566159725189209, 0.4772220849990845, 0.29453104734420776, 0.2846680283546448, 0.7158880233764648, 0.6307367086410522, 0.7177569270133972, 0.7305082678794861, 0.38061749935150146, 0.5700693726539612],
[0.6028605699539185, 0.31891560554504395, 0.4992542862892151, 0.6137638688087463, 0.593483567237854, 0.4768877923488617, 0.620944619178772, 0.45700889825820923, 0.6850719451904297, 0.5408504605293274, 0.6663710474967957, 0.6224966049194336, 0.6836389303207397, 0.6996523141860962, 0.30445340275764465, 0.5081080794334412],
[0.44065308570861816, 0.5895169973373413, 0.36988916993141174, 0.5846110582351685, 0.2704196870326996, 0.4293876886367798, 0.678102970123291, 0.7207025289535522, 0.48189452290534973, 0.28216004371643066, 0.6655344367027283, 0.37909990549087524, 0.6495678424835205, 0.27505993843078613, 0.3400316536426544, 0.6497488617897034],
[0.31407156586647034, 0.6753407120704651, 0.6715640425682068, 0.2771100103855133, 0.31649354100227356, 0.38913488388061523, 0.6466071009635925, 0.5330023765563965, 0.3068549931049347, 0.6439923644065857, 0.713708758354187, 0.3768370449542999, 0.3725622892379761, 0.7118948698043823, 0.6767401099205017, 0.5514048933982849],
[0.3400841951370239, 0.610831618309021, 0.5841134190559387, 0.6386331915855408, 0.7291340827941895, 0.6014353632926941, 0.5727205276489258, 0.537594735622406, 0.3359549939632416, 0.3352784812450409, 0.6684852242469788, 0.39495721459388733, 0.43287450075149536, 0.27805790305137634, 0.6432145237922668, 0.341556191444397],
[0.3019968569278717, 0.5672169327735901, 0.6539328098297119, 0.3903808295726776, 0.3596191704273224, 0.31048551201820374, 0.40814563632011414, 0.5999330282211304, 0.2698666453361511, 0.39824432134628296, 0.47187837958335876, 0.5686739683151245, 0.7000524997711182, 0.6405878663063049, 0.48320433497428894, 0.6846867203712463],
[0.5007346868515015, 0.499371200799942, 0.5008021593093872, 0.4995105564594269, 0.49975404143333435, 0.50089031457901, 0.49987563490867615, 0.5004161596298218, 0.5006093382835388, 0.4993217885494232, 0.5010164976119995, 0.5008881688117981, 0.4998015761375427, 0.5000357627868652, 0.5004066228866577, 0.49890053272247314, 0.5007792711257935, 0.5002681016921997, 0.5006082653999329, 0.5001324415206909, 0.5000895857810974, 0.5007347464561462, 0.5001257061958313, 0.4999212920665741, 0.49874502420425415, 0.4995814859867096, 0.5001711845397949, 0.5008324980735779, 0.49998965859413147, 0.5002251863479614, 0.4994834065437317, 0.5004875063896179],
[0.4827810525894165, 0.5152043700218201, 0.47816064953804016, 0.5013530254364014, 0.45657631754875183, 0.5049551725387573, 0.4577484428882599, 0.4841766953468323, 0.4939885139465332, 0.5495399832725525, 0.44770297408103943, 0.4673631489276886, 0.47598180174827576, 0.49183931946754456, 0.4612504839897156, 0.45250403881073, 0.49229276180267334, 0.5497470498085022, 0.46306177973747253, 0.4618127942085266, 0.5203126668930054, 0.4452832043170929, 0.515855610370636, 0.4768928587436676, 0.44356656074523926, 0.4567354917526245, 0.4475281834602356, 0.47493958473205566, 0.4865424335002899, 0.5360316038131714, 0.47037866711616516, 0.47273486852645874],
[0.5342316627502441, 0.4697716236114502, 0.472364217042923, 0.5399621725082397, 0.5427815318107605, 0.4680992662906647, 0.49255654215812683, 0.5040624141693115, 0.5386728048324585, 0.5295003652572632, 0.4716734290122986, 0.46036744117736816, 0.4871758222579956, 0.45931488275527954, 0.538840115070343, 0.5140937566757202, 0.4980439841747284, 0.5117025375366211, 0.46818095445632935, 0.515372633934021, 0.5423240661621094, 0.5050706267356873, 0.5447957515716553, 0.4707094132900238, 0.4562283754348755, 0.517845094203949, 0.45625507831573486, 0.5159374475479126, 0.46429091691970825, 0.4853436052799225, 0.5352402925491333, 0.4783638119697571],
[0.4964531362056732, 0.4906984269618988, 0.5049418807029724, 0.49854210019111633, 0.5016711354255676, 0.49296772480010986, 0.491257905960083, 0.5106892585754395, 0.4912986755371094, 0.5066453814506531, 0.49300527572631836, 0.4925176203250885, 0.49381595849990845, 0.49088379740715027, 0.49091392755508423, 0.49772822856903076, 0.4920474588871002, 0.4892275333404541, 0.505234956741333, 0.502379834651947, 0.5023471117019653, 0.5032004714012146, 0.49486830830574036, 0.48842841386795044, 0.5028271675109863, 0.4939770996570587, 0.49024102091789246, 0.49537357687950134, 0.4934260845184326, 0.4899829030036926, 0.497679740190506, 0.49428072571754456],
[0.500995934009552, 0.4981738328933716, 0.5015344619750977, 0.5046901106834412, 0.5021976828575134, 0.5056828856468201, 0.49410584568977356, 0.5022515654563904, 0.4948563873767853, 0.4963383376598358, 0.4960510730743408, 0.504841685295105, 0.49679672718048096, 0.49933695793151855, 0.49477025866508484, 0.49914780259132385, 0.5022383332252502, 0.5025632977485657, 0.4969028830528259, 0.5007187128067017, 0.5058197379112244, 0.49662089347839355, 0.505123496055603, 0.4940721094608307, 0.502673864364624, 0.49974822998046875, 0.4986000955104828, 0.4939027726650238, 0.5048823952674866, 0.49872446060180664, 0.5029244422912598, 0.49666860699653625],
[0.49970635771751404, 0.5047701597213745, 0.496538370847702, 0.5007364153862, 0.5050143003463745, 0.4982686936855316, 0.5038818717002869, 0.5047319531440735, 0.4949708878993988, 0.4979841709136963, 0.48966196179389954, 0.4984566867351532, 0.509781539440155, 0.489777147769928, 0.500482439994812, 0.504260778427124, 0.49669361114501953, 0.49847036600112915, 0.5052538514137268, 0.49065297842025757, 0.4994054138660431, 0.4925449788570404, 0.4935077726840973, 0.4954894483089447, 0.5040420293807983, 0.5087926387786865, 0.5053673386573792, 0.49502962827682495, 0.5046442747116089, 0.4969598650932312, 0.4960450828075409, 0.5096070766448975],
[0.4984034299850464, 0.5018622279167175, 0.4995536208152771, 0.5004716515541077, 0.5019991397857666, 0.5010271072387695, 0.500176191329956, 0.49757611751556396, 0.5015305876731873, 0.49903345108032227, 0.4968744218349457, 0.49955514073371887, 0.4995768070220947, 0.5021731853485107, 0.49731680750846863, 0.5028706789016724, 0.5027340054512024, 0.5003503561019897, 0.49950966238975525, 0.5011531114578247, 0.4971843361854553, 0.4990391731262207, 0.4992985427379608, 0.5021290183067322, 0.4993116557598114, 0.49888262152671814, 0.49856680631637573, 0.5002724528312683, 0.499540776014328, 0.4997342824935913, 0.500266432762146, 0.5002633333206177],
[0.5006231665611267, 0.5030871629714966, 0.5111128687858582, 0.5001708269119263, 0.5015509724617004, 0.4945273995399475, 0.5009904503822327, 0.5034040808677673, 0.49790093302726746, 0.5075419545173645, 0.49976646900177, 0.5092307329177856, 0.5073802471160889, 0.5101233124732971, 0.5039172172546387, 0.5025115013122559, 0.4951685965061188, 0.4928435683250427, 0.5097711682319641, 0.49311864376068115, 0.49663057923316956, 0.49379661679267883, 0.5041985511779785, 0.49127089977264404, 0.49985766410827637, 0.4895881712436676, 0.507758617401123, 0.49229052662849426, 0.4968516230583191, 0.4987664520740509, 0.49963393807411194, 0.5059248805046082],
[0.4999612867832184, 0.5000916719436646, 0.5000242590904236, 0.5003541707992554, 0.5000703930854797, 0.5002297163009644, 0.5001067519187927, 0.5000209808349609, 0.5000007152557373, 0.4998440444469452, 0.4999513328075409, 0.4999980926513672, 0.49993911385536194, 0.5002784729003906, 0.5001875758171082, 0.4998072683811188, 0.49990537762641907, 0.500083327293396, 0.5000559091567993, 0.49998003244400024, 0.500012218952179, 0.49982425570487976, 0.5001369118690491, 0.5002649426460266, 0.5000676512718201, 0.5000616908073425, 0.5001186728477478, 0.5003058314323425, 0.5000393390655518, 0.4998978078365326, 0.5001875758171082, 0.49973833560943604],
[0.4796643555164337, 0.4498644471168518, 0.5419439673423767, 0.4984651803970337, 0.5330570936203003, 0.5285122394561768, 0.48368939757347107, 0.5249159932136536, 0.5313071012496948, 0.48224976658821106, 0.5014405846595764, 0.47617945075035095, 0.5359095931053162, 0.5193734765052795, 0.5035406351089478, 0.4847269654273987, 0.518431544303894, 0.49224653840065, 0.5194826722145081, 0.483934223651886, 0.5084213614463806, 0.5002811551094055, 0.45730406045913696, 0.4695410132408142, 0.5205843448638916, 0.46093031764030457, 0.47950509190559387, 0.46936288475990295, 0.46317973732948303, 0.502869188785553, 0.4846448600292206, 0.5276190638542175],
[0.49466025829315186, 0.4912324845790863, 0.49086302518844604, 0.48590609431266785, 0.49171581864356995, 0.49057871103286743, 0.4895901381969452, 0.49976372718811035, 0.4809147119522095, 0.502402126789093, 0.5042422413825989, 0.4978126883506775, 0.5105680227279663, 0.5005140900611877, 0.507940948009491, 0.4926835596561432, 0.4986056983470917, 0.4929129481315613, 0.5125876665115356, 0.5052323937416077, 0.5023540258407593, 0.49892640113830566, 0.5192012786865234, 0.5020356178283691, 0.5076141953468323, 0.5045788884162903, 0.49185341596603394, 0.5075315237045288, 0.5015260577201843, 0.48808133602142334, 0.5047168731689453, 0.49879056215286255],
[0.5135540962219238, 0.5159333944320679, 0.4966016113758087, 0.5076066851615906, 0.4927476942539215, 0.5012710094451904, 0.5071384906768799, 0.4937516748905182, 0.4966219365596771, 0.5240035057067871, 0.5028460621833801, 0.48359331488609314, 0.5088754296302795, 0.5074639916419983, 0.48441362380981445, 0.49400609731674194, 0.4940037131309509, 0.4829639792442322, 0.4933110773563385, 0.49927079677581787, 0.4882628917694092, 0.49766474962234497, 0.5199747085571289, 0.5038765072822571, 0.49823248386383057, 0.5078905820846558, 0.502973735332489, 0.4910129904747009, 0.4759645462036133, 0.5016224384307861, 0.5160841345787048, 0.5153247714042664],
[0.48346737027168274, 0.5126169919967651, 0.494063138961792, 0.5048945546150208, 0.512718677520752, 0.48604437708854675, 0.4948023557662964, 0.5152535438537598, 0.48922792077064514, 0.48567837476730347, 0.5008851885795593, 0.4981063902378082, 0.5107318758964539, 0.5121833086013794, 0.4974450170993805, 0.5160024166107178, 0.5067003965377808, 0.5095535516738892, 0.5178621411323547, 0.5184922814369202, 0.4842585325241089, 0.49026575684547424, 0.4826575815677643, 0.5117677450180054, 0.5045148730278015, 0.5010201930999756, 0.4853515028953552, 0.49022650718688965, 0.5196285843849182, 0.4983561038970947, 0.5194758176803589, 0.4994135797023773],
[0.5007563233375549, 0.5209970474243164, 0.4910847544670105, 0.5118230581283569, 0.518443763256073, 0.4860040545463562, 0.5038061141967773, 0.4961855411529541, 0.47364354133605957, 0.4973774552345276, 0.5184009671211243, 0.48350903391838074, 0.48068398237228394, 0.5124446153640747, 0.49620702862739563, 0.4936774969100952, 0.49198925495147705, 0.5220075845718384, 0.49791210889816284, 0.4996306300163269, 0.49136972427368164, 0.4761165380477905, 0.5018633604049683, 0.4964029788970947, 0.4897874593734741, 0.5179698467254639, 0.49146807193756104, 0.487003356218338, 0.48635396361351013, 0.5060707330703735, 0.4824448823928833, 0.5111345648765564],
[0.4914764165878296, 0.5191153287887573, 0.4860236346721649, 0.521428108215332, 0.49712684750556946, 0.4849804639816284, 0.49562498927116394, 0.5034244060516357, 0.5214207768440247, 0.5223657488822937, 0.48413360118865967, 0.4978121519088745, 0.5095353126525879, 0.5152540802955627, 0.49553340673446655, 0.475879043340683, 0.5100280046463013, 0.4816436469554901, 0.48816344141960144, 0.4891544580459595, 0.5094245076179504, 0.48106756806373596, 0.5224123001098633, 0.48811206221580505, 0.5090351104736328, 0.4797230362892151, 0.4987039566040039, 0.48147597908973694, 0.5064705610275269, 0.5038074254989624, 0.49613919854164124, 0.4921214282512665],
[0.4681971073150635, 0.519610583782196, 0.47401225566864014, 0.5292559862136841, 0.5281692147254944, 0.49781107902526855, 0.5061318278312683, 0.5192916989326477, 0.49844759702682495, 0.5178854465484619, 0.4715655744075775, 0.4864337742328644, 0.4724082946777344, 0.46626758575439453, 0.5355581045150757, 0.4982205927371979, 0.5087517499923706, 0.4915730357170105, 0.5264003872871399, 0.5054311752319336, 0.4874545931816101, 0.47994184494018555, 0.5231049060821533, 0.5262877941131592, 0.4556920826435089, 0.4683200716972351, 0.5351901650428772, 0.4879281222820282, 0.5313090085983276, 0.4885192811489105, 0.5409817099571228, 0.4889790415763855],
[0.5016286373138428, 0.49865713715553284, 0.5042300224304199, 0.5024093389511108, 0.4989064037799835, 0.5001581907272339, 0.5008576512336731, 0.5018376708030701, 0.502752423286438, 0.5016847252845764, 0.4960516095161438, 0.49625498056411743, 0.4984109401702881, 0.4985446333885193, 0.5035771131515503, 0.5019975900650024, 0.5016998052597046, 0.4976736307144165, 0.497020959854126, 0.5019813179969788, 0.499740332365036, 0.5005195140838623, 0.5003218650817871, 0.4961792528629303, 0.4964878559112549, 0.4979749917984009, 0.5013660788536072, 0.49968549609184265, 0.499072402715683, 0.49803104996681213, 0.4982646107673645, 0.502326488494873],
[0.4798681139945984, 0.5101765990257263, 0.4810822606086731, 0.5079861283302307, 0.4938918352127075, 0.48804590106010437, 0.5037848949432373, 0.48952537775039673, 0.4790498614311218, 0.4911949634552002, 0.490415096282959, 0.48237085342407227, 0.5080506205558777, 0.5188248753547668, 0.4900379478931427, 0.5171994566917419, 0.5232212543487549, 0.49182215332984924, 0.5024696588516235, 0.5121249556541443, 0.517812192440033, 0.4913305640220642, 0.5101326704025269, 0.48801156878471375, 0.5071079730987549, 0.5079945921897888, 0.4795791506767273, 0.48683759570121765, 0.5216249227523804, 0.5142247676849365, 0.4841814339160919, 0.5146210193634033],
]


n = (args.depth - 2) // 6
cfg = [[16]*n, [32]*n, [32]*n]
cfg = [item for sub_list in cfg for item in sub_list]

layer_id = 1
im = 0

cfg_mask1 = []
cfg_mask2 = []
for m in model.modules():
    if isinstance(m, nn.Conv2d):
        if layer_id == 1:
            layer_id = layer_id + 1
            continue
        if layer_id % 2 == 0:
            out_channels = m.weight.data.shape[0]
            layer = layer_id // 2 - 1
            if out_channels == cfg[layer]:
                cfg_mask1.append(torch.ones(out_channels))
                layer_id += 1
                continue

            weight_copy = m.weight.data.abs().clone()
            weight_copy = weight_copy.cpu().numpy()
            L1_norm = np.sum(weight_copy, axis=(1, 2, 3))
            arg_max = np.argsort(L1_norm)
            arg_max_rev = arg_max[::-1][:cfg[layer]]
            assert arg_max_rev.size == cfg[layer], "size of arg_max_rev not correct"
            mask = torch.zeros(out_channels)
            mask[arg_max_rev.tolist()] = 1
            cfg_mask1.append(mask)
            layer_id += 1
            continue
            
        if layer_id % 2 == 1:
            out_channels = m.weight.data.shape[0]
            imscore_copy = imscore[im]
            arg_max = np.argsort(imscore_copy)
            layer = layer_id // 2 - 1
            m = out_channels - cfg[layer]
            arg_max_rev = arg_max[m:]
            assert arg_max_rev.size == cfg[layer], "size of arg_max_rev not correct"            
            mask = torch.zeros(out_channels)
            mask[arg_max_rev.tolist()] = 1                      
            cfg_mask2.append(mask.clone())                       # cfg_mask是每一层具体剩下哪几个通道
            layer_id += 1
            im += 1
            continue
        layer_id += 1

newmodel = models.__dict__[args.arch](depth=args.depth, dataset=args.dataset, cfg=cfg)

print(newmodel)
if args.cuda:
    newmodel.cuda()


layer_id_in_cfg = 0
conv_count = 1


for [m0, m1] in zip(model.modules(), newmodel.modules()):
    if isinstance(m0, nn.Conv2d):                  
        if conv_count == 1:
            m1.weight.data = m0.weight.data.clone()
            conv_count += 1
            continue
        
        if conv_count in [20, 38]:
            mask = cfg_mask1[layer_id_in_cfg-1]
            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
            if idx.size == 1:
                idx = np.resize(idx, (1,))
            w = m0.weight.data[:, idx.tolist(), :, :].clone()
            m1.weight.data = w.clone()
            conv_count += 1
            continue            
        
        if conv_count % 2 == 0:
            mask = cfg_mask1[layer_id_in_cfg]
            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
            if idx.size == 1:
                idx = np.resize(idx, (1,))
            w = m0.weight.data[:, idx.tolist(), :, :].clone()
            m1.weight.data = w.clone()
            conv_count += 1
            continue            
        
        if conv_count % 2 == 1:
            mask = cfg_mask2[layer_id_in_cfg]
            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
            if idx.size == 1:
                idx = np.resize(idx, (1,))
            w = m0.weight.data[idx.tolist(), :, :, :].clone()
            m1.weight.data = w.clone()
            layer_id_in_cfg += 1
            conv_count += 1
            continue

    elif isinstance(m0, nn.BatchNorm2d):
        if conv_count == 2:
            m1.weight.data = m0.weight.data.clone()
            m1.bias.data = m0.bias.data.clone()
            m1.running_mean = m0.running_mean.clone()
            m1.running_var = m0.running_var.clone()            
            continue
        if conv_count % 2 == 1:
            m1.weight.data = m0.weight.data.clone()
            m1.bias.data = m0.bias.data.clone()
            m1.running_mean = m0.running_mean.clone()
            m1.running_var = m0.running_var.clone()
            continue
        if conv_count % 2 == 0:
            mask = cfg_mask2[layer_id_in_cfg-1]
            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
            if idx.size == 1:
                idx = np.resize(idx, (1,))
            m1.weight.data = m0.weight.data[idx.tolist()].clone()
            m1.bias.data = m0.bias.data[idx.tolist()].clone()
            m1.running_mean = m0.running_mean[idx.tolist()].clone()
            m1.running_var = m0.running_var[idx.tolist()].clone()
            continue       

    elif isinstance(m0, nn.Linear):
        m.weight.data.normal_(0, 0.01)
        m.bias.data.zero_()



torch.save({'cfg': cfg, 'state_dict': newmodel.state_dict()}, os.path.join(args.save, 'res56pruned.pth.tar'))

num_parameters = sum([param.nelement() for param in newmodel.parameters()])
print(newmodel)
model = newmodel
acc = test(model)
acc=acc.numpy()

print("number of parameters: "+str(num_parameters))
with open(os.path.join(args.save, "res56pruned.txt"), "w") as fp:
    fp.write("Number of parameters: \n"+str(num_parameters)+"\n")
    fp.write("Test accuracy: \n"+str(acc)+"\n")

with torch.cuda.device(0):
  net = model
  flops, params = get_model_complexity_info(net, (3, 32,32), as_strings=True, print_per_layer_stat=True)
  print('Flops:  ' + flops)
  print('Params: ' + params)