# -*- coding: utf-8 -*-
"""
Created on Thu Oct 17 09:12:53 2019

@author: ASUS
"""

import argparse
import numpy as np
import os

import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
from torch.autograd import Variable
from torchvision import datasets, transforms

import models.cifar3 as models
from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig
from ptflops import get_model_complexity_info

# Prune settings 
parser = argparse.ArgumentParser(description='PyTorch Slimming CIFAR prune')
parser.add_argument('--dataset', type=str, default='cifar10',
                    help='training dataset (default: cifar10)')
parser.add_argument('--test-batch-size', type=int, default=10000, metavar='N',
                    help='input batch size for testing (default: 256)')
parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='disables CUDA training')
parser.add_argument('--depth', type=int, default=56,
                    help='depth of the resnet')
parser.add_argument('--model', default='./results-resnet/model_best.pth.tar', type=str, metavar='PATH',
                    help='path to the model (default: none)')
parser.add_argument('--resume', default='', type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('--save', default='.', type=str, metavar='PATH',
                    help='path to save pruned model (default: none)')
parser.add_argument('--arch', default='resnet', type=str, 
                    help='architecture to use')
parser.add_argument('-v', default='A', type=str, 
                    help='version of the model')

args = parser.parse_args()

state = {k: v for k, v in args._get_kwargs()}

args.cuda = not args.no_cuda and torch.cuda.is_available()

if not os.path.exists(args.save):
    os.makedirs(args.save)

model = models.__dict__[args.arch](dataset=args.dataset, depth=args.depth)

if args.cuda:
    model.cuda()

print(model)



if args.model:
    if os.path.isfile(args.model):
        print("=> loading checkpoint '{}'".format(args.model))
        checkpoint = torch.load(args.model)
        args.start_epoch = checkpoint['epoch']
        best_acc = checkpoint['best_acc']
        model.load_state_dict(checkpoint['state_dict'])
        print("=> loaded checkpoint '{}' (epoch {}) best_acc: {:f}"
              .format(args.model, checkpoint['epoch'], best_acc))
    else:
        print("=> no checkpoint found at '{}'".format(args.resume))

print('Pre-processing Successful!')

# simple test model after Pre-processing prune (simple set BN scales to zeros)
def test(model):
    kwargs = {'num_workers': 0, 'pin_memory': True} if args.cuda else {}
    if args.dataset == 'cifar10':
        test_loader = torch.utils.data.DataLoader(
            datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),
            batch_size=args.test_batch_size, shuffle=False, **kwargs)
    elif args.dataset == 'cifar100':
        test_loader = torch.utils.data.DataLoader(
            datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),
            batch_size=args.test_batch_size, shuffle=False, **kwargs)
    else:
        raise ValueError("No valid dataset is given.")

    correct = 0
    for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        with torch.no_grad():
            data, target = Variable(data), Variable(target)
            output = model(data)
            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
            correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    print('\nTest set: Accuracy: {}/{} ({:.1f}%)\n'.format(
        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))
    return 10000 * correct / float(len(test_loader.dataset))

acc = test(model)
acc=acc.numpy()

num_parameters = sum([param.nelement() for param in model.parameters()])
print("number of parameters: "+str(num_parameters)+"\n")
with open(os.path.join(args.save, "resprune.txt"), "w") as fp:
    fp.write("Number of parameters: \n"+str(num_parameters)+"\n")
    fp.write("Test accuracy: \n"+str(acc)+"\n")

# 计算模型参数
with torch.cuda.device(0):
  net = model
  flops, params = get_model_complexity_info(net, (3, 32,32), as_strings=True, print_per_layer_stat=True)
  print('Flops:  ' + flops)
  print('Params: ' + params)


#******************************************************************************************************#

# 模型裁剪

imscore = [
[4.822144985198975, 5.045901775360107, 5.030910015106201, 4.857987880706787, 4.882938861846924, 4.914138317108154, 4.802637577056885, 4.773819446563721, 4.8783440589904785, 5.032192230224609, 4.951292037963867, 4.920350074768066, 5.087520599365234, 4.934971809387207, 4.811524868011475, 4.863312244415283],
[4.853515148162842, 5.006761074066162, 5.002885818481445, 4.900691986083984, 4.8844451904296875, 4.9247331619262695, 4.859967231750488, 4.752659797668457, 4.903903484344482, 5.025936126708984, 4.965812683105469, 5.050684452056885, 4.982091903686523, 4.95603084564209, 4.845692157745361, 4.839256286621094],
[4.926877498626709, 5.08827018737793, 5.18213415145874, 4.9199628829956055, 4.988259315490723, 4.996641635894775, 4.894207954406738, 4.880859851837158, 4.961411476135254, 5.101062297821045, 4.966198444366455, 5.021120071411133, 5.014156341552734, 4.931672096252441, 4.917117595672607, 4.91536283493042],
[5.037084579467773, 5.029768943786621, 5.11191463470459, 4.943403720855713, 5.22272253036499, 5.068365573883057, 4.907251834869385, 4.939412593841553, 5.082545757293701, 5.104031085968018, 5.048951625823975, 5.126983165740967, 5.073119163513184, 4.99464225769043, 4.993589878082275, 5.0102338790893555],
[4.966315746307373, 4.964301109313965, 5.031167984008789, 4.910074234008789, 5.151367664337158, 5.0860466957092285, 4.8828444480896, 4.965701580047607, 5.107445240020752, 5.092288017272949, 5.0960693359375, 5.051754951477051, 4.940930366516113, 4.9849443435668945, 4.953064441680908, 5.229968070983887],
[4.944296836853027, 4.98336935043335, 5.046846866607666, 4.904287338256836, 4.984951496124268, 5.024817943572998, 4.845419883728027, 5.066384315490723, 5.08009147644043, 5.123535633087158, 5.1596503257751465, 5.050313472747803, 4.952812194824219, 4.932052135467529, 5.000095844268799, 5.2249908447265625],
[4.972334861755371, 5.128894805908203, 5.056710243225098, 4.941020488739014, 5.011659622192383, 5.027710437774658, 4.86277437210083, 4.983844757080078, 4.985470771789551, 5.071393013000488, 5.100601673126221, 4.990612506866455, 4.941503524780273, 4.948184013366699, 4.975987911224365, 5.004977226257324],
[4.919938564300537, 4.985353469848633, 5.004383087158203, 4.923874378204346, 4.948279857635498, 5.058586597442627, 4.926757335662842, 4.944669723510742, 4.931519985198975, 5.175751209259033, 5.193778038024902, 5.128537178039551, 4.941888332366943, 4.965700149536133, 4.949530124664307, 5.144912242889404],
[5.112274646759033, 5.031167507171631, 5.263554573059082, 4.948207378387451, 4.938427925109863, 5.078132629394531, 4.9100799560546875, 5.020021438598633, 4.944645881652832, 5.102936744689941, 5.026005268096924, 5.039068222045898, 4.952710151672363, 4.942955017089844, 5.243781089782715, 5.475003719329834],
[4.6893391609191895, 4.364216327667236, 4.017813205718994, 4.311422348022461, 4.62265682220459, 4.450912952423096, 5.701013565063477, 4.199123382568359, 3.533125400543213, 3.9387974739074707, 4.451797962188721, 4.472762584686279, 3.855837106704712, 4.46812105178833, 3.815333366394043, 3.759413957595825, 5.432631015777588, 4.970274448394775, 4.4133687019348145, 5.006814002990723, 5.139626502990723, 4.806204319000244, 4.911751747131348, 4.602921009063721, 5.911715507507324, 5.317033767700195, 4.872329235076904, 4.890238285064697, 4.8368330001831055, 5.85460901260376, 4.8702921867370605, 4.809340953826904],
[4.999999046325684, 4.999997138977051, 4.999996662139893, 4.999988555908203, 4.999990463256836, 4.999998569488525, 4.999998092651367, 4.999999523162842, 4.99999475479126, 4.999995231628418, 4.99999475479126, 4.99998664855957, 4.99999475479126, 4.999997615814209, 4.99999475479126, 4.999986171722412, 4.999999523162842, 4.999999523162842, 4.999998569488525, 4.999999046325684, 4.999999523162842, 4.999996662139893, 4.999999523162842, 5.000000476837158, 4.999999523162842, 4.999999046325684, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 5.000000476837158, 5.000001907348633],
[4.999999523162842, 5.0, 4.999999523162842, 5.0, 4.999999523162842, 4.999999523162842, 5.000000476837158, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 5.000000476837158, 5.0, 4.999999523162842, 5.000000476837158, 4.999999523162842, 4.999999523162842, 5.0, 5.0, 5.000000476837158, 5.000000476837158, 5.000000476837158, 5.000000476837158, 4.999999523162842, 4.999999523162842, 5.000000476837158],
[5.000000476837158, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 5.000000476837158, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 5.0, 5.0, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 4.999999523162842, 5.000000476837158, 4.999999523162842, 5.000000476837158, 4.999999523162842],
[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0],
[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0],
[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0],
[5.000000476837158, 5.000000476837158, 5.0, 4.999999523162842, 5.0, 5.0, 5.0, 5.0, 5.0, 4.999999523162842, 5.000000476837158, 5.0, 5.0, 5.0, 4.999999523162842, 5.0, 5.0, 5.000000476837158, 5.000000476837158, 5.000000476837158, 4.999999523162842, 5.0, 5.0, 5.0, 5.0, 5.000000476837158, 5.0, 5.0, 5.000000476837158, 5.0, 5.0, 5.0],
[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0],
[4.86480188369751, 5.315568923950195, 4.560555458068848, 4.4347243309021, 5.038243770599365, 4.161726474761963, 5.351122856140137, 4.458191871643066, 4.740711212158203, 4.485919952392578, 5.327592849731445, 4.240474224090576, 5.1731977462768555, 5.077552795410156, 4.479516506195068, 4.478841304779053, 4.434322834014893, 4.9986348152160645, 4.974730014801025, 4.65043306350708, 5.046872615814209, 3.9926629066467285, 4.581600189208984, 5.347588539123535, 4.318835735321045, 4.701189994812012, 4.899055004119873, 5.091355323791504, 4.689756393432617, 5.249174118041992, 4.661303520202637, 4.899473190307617, 5.043020248413086, 4.798083305358887, 5.772194862365723, 4.938113689422607, 4.776060104370117, 5.195521354675293, 4.578769207000732, 5.019444465637207, 4.987508773803711, 5.243867874145508, 5.386183738708496, 4.4584641456604, 5.102049827575684, 4.758658409118652, 4.871547222137451, 5.304232120513916, 4.542277812957764, 5.658523082733154, 5.721526622772217, 4.140368938446045, 5.692852973937988, 5.144056797027588, 4.558535099029541, 5.17710018157959, 5.596709251403809, 4.891280651092529, 4.954293251037598, 5.2609686851501465, 5.388831615447998, 4.801537990570068, 5.3769683837890625, 5.193605422973633],
[2.988694429397583, 5.397462844848633, 3.636587381362915, 2.581590414047241, 3.744208335876465, 3.4615933895111084, 3.4231061935424805, 2.500976324081421, 2.2525272369384766, 4.212885856628418, 4.789222717285156, 2.5408260822296143, 2.7856926918029785, 4.473891735076904, 4.618359088897705, 2.0547029972076416, 2.927250623703003, 4.14599084854126, 3.843163013458252, 3.4913811683654785, 3.331902027130127, 3.95559024810791, 4.4151225090026855, 3.3233282566070557, 3.287846565246582, 4.186756610870361, 4.7909088134765625, 4.872627258300781, 2.4673027992248535, 5.049269676208496, 5.878671169281006, 5.491570949554443, 2.683138608932495, 3.835071325302124, 5.851559638977051, 4.188862323760986, 4.1835503578186035, 5.5010552406311035, 5.2492265701293945, 5.499295234680176, 4.184128284454346, 2.718075752258301, 2.5741283893585205, 4.028273582458496, 2.9045658111572266, 4.916627407073975, 3.4639875888824463, 5.650467395782471, 4.888552188873291, 5.624392509460449, 2.6046805381774902, 5.230442523956299, 4.413049221038818, 3.040102005004883, 3.807976007461548, 3.929330825805664, 4.305778503417969, 4.317613124847412, 2.882169008255005, 4.6354570388793945, 4.987468242645264, 3.990476131439209, 4.466408729553223, 5.331765174865723],
[4.2265543937683105, 4.758796691894531, 4.795709133148193, 4.382291316986084, 4.395437717437744, 4.325562953948975, 4.495739459991455, 4.453003406524658, 5.104791164398193, 5.152386665344238, 5.033353805541992, 3.785127639770508, 4.6803507804870605, 4.821637153625488, 4.125542640686035, 4.797368049621582, 4.84677791595459, 4.809305667877197, 4.664013862609863, 4.354269504547119, 4.8655900955200195, 4.953039646148682, 4.689609527587891, 4.5282697677612305, 4.935825347900391, 4.962922096252441, 4.618659496307373, 4.951820373535156, 4.953619480133057, 4.809577941894531, 5.285706520080566, 5.137111663818359, 4.838443279266357, 4.866033554077148, 5.218107223510742, 4.488868236541748, 4.698715686798096, 4.854742050170898, 5.026925563812256, 5.389121055603027, 4.685044288635254, 4.7728729248046875, 5.000612735748291, 4.723995208740234, 4.866403579711914, 5.232644081115723, 4.819957733154297, 4.906109809875488, 4.8206071853637695, 4.955306053161621, 4.905596733093262, 4.682900905609131, 5.04013204574585, 4.503457069396973, 5.048008441925049, 4.879595756530762, 4.5190324783325195, 5.027929782867432, 4.5993781089782715, 4.850645065307617, 5.323427200317383, 4.7122392654418945, 5.186402320861816, 5.193551063537598],
[4.392230033874512, 3.47247314453125, 5.072165489196777, 4.152556896209717, 3.3912062644958496, 4.1258697509765625, 4.040135383605957, 4.41031551361084, 4.9824934005737305, 6.036999702453613, 5.318334579467773, 4.270444869995117, 2.9414563179016113, 5.847777366638184, 3.4121885299682617, 3.906001329421997, 4.86682653427124, 4.352771759033203, 3.972966194152832, 3.26627254486084, 4.220706939697266, 3.1692428588867188, 4.1147780418396, 3.9100186824798584, 4.152957439422607, 5.695718765258789, 3.507009983062744, 5.067470550537109, 5.195125102996826, 3.7992379665374756, 5.388421535491943, 5.405606746673584, 2.9081363677978516, 4.381603717803955, 5.380339622497559, 3.3335957527160645, 3.7697935104370117, 5.088768482208252, 4.470297813415527, 5.984339237213135, 3.541954755783081, 3.486795663833618, 5.038112163543701, 4.220095634460449, 4.693637371063232, 5.401861190795898, 4.961128234863281, 5.405153274536133, 4.12912130355835, 5.91820764541626, 5.283349990844727, 5.301390647888184, 5.455851078033447, 5.015778064727783, 5.502054214477539, 5.773767471313477, 3.4391229152679443, 4.20338773727417, 4.025809288024902, 4.400652885437012, 4.534815311431885, 4.187589168548584, 3.661019802093506, 4.759940147399902],
[3.4375429153442383, 3.441462755203247, 5.058074474334717, 4.571289539337158, 2.8621881008148193, 2.8836963176727295, 3.892291307449341, 4.2892374992370605, 3.8815553188323975, 5.538064479827881, 5.853242874145508, 3.9098284244537354, 3.4654552936553955, 5.411754131317139, 3.2073512077331543, 3.513855457305908, 4.398654937744141, 4.36339807510376, 3.1170132160186768, 4.400252342224121, 4.903954029083252, 5.885430812835693, 5.37150239944458, 4.306262493133545, 3.4152815341949463, 3.598421096801758, 3.410210371017456, 5.573724269866943, 4.280951023101807, 4.5922980308532715, 4.837244510650635, 6.252702713012695, 5.992386341094971, 4.897620677947998, 2.294989585876465, 5.439122200012207, 4.199244976043701, 5.070326328277588, 4.260891437530518, 6.204944610595703, 4.336696147918701, 5.444207191467285, 5.66054630279541, 2.8882856369018555, 5.185052871704102, 4.419665813446045, 4.806730270385742, 5.448347568511963, 4.248324871063232, 4.2619757652282715, 2.8879921436309814, 5.025717258453369, 5.274361610412598, 4.1104302406311035, 3.3545730113983154, 4.695094108581543, 5.230923652648926, 3.78971791267395, 2.8158206939697266, 3.6916980743408203, 4.434414386749268, 4.798643112182617, 4.063281059265137, 4.104421615600586],
[4.958693981170654, 4.9618048667907715, 4.988190650939941, 4.996314525604248, 4.965608596801758, 4.981152534484863, 4.995921611785889, 4.974623203277588, 5.019242763519287, 4.987088680267334, 5.002167701721191, 4.930185317993164, 4.973014831542969, 5.011893272399902, 4.960292816162109, 4.984313011169434, 5.006009101867676, 4.995421409606934, 4.98474645614624, 5.013172149658203, 4.994051933288574, 4.97504186630249, 4.990678787231445, 4.96974515914917, 4.970727920532227, 5.003424167633057, 4.978438377380371, 5.008558750152588, 5.01296854019165, 5.012642860412598, 5.004289150238037, 5.012414932250977, 4.966902256011963, 4.996893405914307, 4.9610209465026855, 4.95474910736084, 4.958578109741211, 4.966475009918213, 4.973690032958984, 4.960636615753174, 4.949837684631348, 4.993602752685547, 4.9546027183532715, 4.976065635681152, 4.987102031707764, 5.000813961029053, 4.99288272857666, 4.980529308319092, 4.975458145141602, 5.025366306304932, 4.981473922729492, 5.002984046936035, 4.9820051193237305, 4.977243423461914, 4.9805378913879395, 4.96798038482666, 4.988673210144043, 4.963382244110107, 4.937077045440674, 4.971613883972168, 5.028659820556641, 4.994987487792969, 4.999912738800049, 4.972231388092041],
[3.607633590698242, 2.735502243041992, 6.049358367919922, 5.726251125335693, 3.0481605529785156, 5.523210525512695, 3.9813954830169678, 5.760496139526367, 6.185470104217529, 6.326962471008301, 3.171013593673706, 3.762058973312378, 5.808970928192139, 3.7496042251586914, 3.759998083114624, 4.075498104095459, 6.254678726196289, 5.72684907913208, 4.175591468811035, 3.1654863357543945, 6.465963363647461, 5.599817276000977, 3.573869228363037, 6.087152004241943, 4.268918037414551, 6.16174840927124, 5.518289566040039, 5.905567169189453, 3.905484914779663, 4.102788925170898, 6.202836513519287, 3.4125187397003174, 3.1418087482452393, 4.474543571472168, 5.97385311126709, 3.1316659450531006, 4.308231830596924, 4.093717575073242, 3.3233225345611572, 5.831982135772705, 2.994858503341675, 3.231290102005005, 3.9648261070251465, 3.7470107078552246, 3.23917818069458, 4.722909450531006, 3.543621301651001, 6.1630635261535645, 4.31298828125, 5.2616729736328125, 3.4625601768493652, 5.665750026702881, 5.923142910003662, 5.769695281982422, 3.191263437271118, 5.964200496673584, 5.887693405151367, 6.054741382598877, 3.1546733379364014, 5.7507524490356445, 3.7603437900543213, 3.555262565612793, 3.103888511657715, 5.693838596343994],
[4.985004425048828, 4.545196056365967, 6.047365665435791, 5.721354961395264, 5.487709999084473, 3.909860849380493, 3.221816062927246, 5.300453186035156, 5.109374046325684, 4.55849027633667, 3.924020290374756, 3.4981367588043213, 3.4697232246398926, 6.387969493865967, 3.7754013538360596, 4.832998752593994, 5.731088638305664, 5.967222213745117, 5.422945499420166, 6.44764518737793, 5.842711925506592, 5.273268699645996, 4.578165531158447, 6.3432936668396, 4.983803749084473, 4.316027641296387, 5.706926345825195, 4.837629318237305, 4.556624412536621, 5.1193318367004395, 5.502405166625977, 5.1554365158081055, 3.317763090133667, 5.530918121337891, 5.120408058166504, 4.260135173797607, 3.3098268508911133, 3.3814642429351807, 2.7455122470855713, 2.919614791870117, 2.5532326698303223, 4.8818583488464355, 4.470022678375244, 2.669421434402466, 5.195574760437012, 4.115480422973633, 5.22838830947876, 2.629331350326538, 4.34283447265625, 4.818798065185547, 5.836843967437744, 5.7048187255859375, 3.6423282623291016, 3.472594976425171, 4.189207553863525, 3.7983310222625732, 4.629448890686035, 4.348616600036621, 2.367825746536255, 2.1846208572387695, 4.847148895263672, 4.72735071182251, 5.7355499267578125, 4.649938106536865],
[9.25227165222168, 9.269556045532227, 9.55249309539795, 9.280237197875977, 9.368288040161133, 8.982563018798828, 9.184541702270508, 9.20341968536377, 9.399493217468262, 9.484829902648926, 9.360173225402832, 9.434844970703125, 9.301525115966797, 9.418469429016113, 9.300495147705078, 9.15945053100586, 9.448493003845215, 9.045401573181152, 9.46377182006836, 9.16299057006836, 8.829859733581543, 9.081329345703125, 8.882701873779297, 9.177162170410156, 9.388667106628418, 9.495238304138184, 9.402922630310059, 9.353214263916016, 8.888345718383789, 9.237330436706543, 9.167490005493164, 9.105554580688477, 9.431954383850098, 9.265756607055664, 9.610645294189453, 9.429717063903809, 9.423650741577148, 9.577872276306152, 9.582085609436035, 9.641600608825684, 9.600603103637695, 9.492562294006348, 9.514214515686035, 9.549272537231445, 9.535489082336426, 9.39992618560791, 9.157916069030762, 9.466907501220703, 9.419668197631836, 9.492026329040527, 9.530597686767578, 9.096634864807129, 9.415915489196777, 9.509766578674316, 9.513436317443848, 9.456429481506348, 9.537507057189941, 9.461746215820312, 9.508899688720703, 9.456488609313965, 9.24079418182373, 8.978594779968262, 9.474328994750977, 9.597650527954102],
]


n = (args.depth - 2) // 6
cfg = [[16]*n, [32]*n, [32]*n]
cfg = [item for sub_list in cfg for item in sub_list]

layer_id = 1
im = 0

cfg_mask1 = []
cfg_mask2 = []
for m in model.modules():
    if isinstance(m, nn.Conv2d):
        if layer_id == 1:
            layer_id = layer_id + 1
            continue
        if layer_id % 2 == 0:
            out_channels = m.weight.data.shape[0]
            layer = layer_id // 2 - 1
            if out_channels == cfg[layer]:
                cfg_mask1.append(torch.ones(out_channels))
                layer_id += 1
                continue

            weight_copy = m.weight.data.abs().clone()
            weight_copy = weight_copy.cpu().numpy()
            L1_norm = np.sum(weight_copy, axis=(1, 2, 3))
            arg_max = np.argsort(L1_norm)
            arg_max_rev = arg_max[::-1][:cfg[layer]]
            assert arg_max_rev.size == cfg[layer], "size of arg_max_rev not correct"
            mask = torch.zeros(out_channels)
            mask[arg_max_rev.tolist()] = 1
            cfg_mask1.append(mask)
            layer_id += 1
            continue
            
        if layer_id % 2 == 1:
            out_channels = m.weight.data.shape[0]
            imscore_copy = imscore[im]
            arg_max = np.argsort(imscore_copy)
            layer = layer_id // 2 - 1
            m = out_channels - cfg[layer]
            arg_max_rev = arg_max[m:]
            assert arg_max_rev.size == cfg[layer], "size of arg_max_rev not correct"            
            mask = torch.zeros(out_channels)
            mask[arg_max_rev.tolist()] = 1                      
            cfg_mask2.append(mask.clone())                       # cfg_mask是每一层具体剩下哪几个通道
            layer_id += 1
            im += 1
            continue
        layer_id += 1

newmodel = models.__dict__[args.arch](depth=args.depth, dataset=args.dataset, cfg=cfg)

print(newmodel)
if args.cuda:
    newmodel.cuda()


layer_id_in_cfg = 0
conv_count = 1


for [m0, m1] in zip(model.modules(), newmodel.modules()):
    if isinstance(m0, nn.Conv2d):                  
        if conv_count == 1:
            m1.weight.data = m0.weight.data.clone()
            conv_count += 1
            continue
        
        if conv_count in [20, 38]:
            mask = cfg_mask1[layer_id_in_cfg-1]
            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
            if idx.size == 1:
                idx = np.resize(idx, (1,))
            w = m0.weight.data[:, idx.tolist(), :, :].clone()
            m1.weight.data = w.clone()
            conv_count += 1
            continue            
        
        if conv_count % 2 == 0:
            mask = cfg_mask1[layer_id_in_cfg]
            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
            if idx.size == 1:
                idx = np.resize(idx, (1,))
            w = m0.weight.data[:, idx.tolist(), :, :].clone()
            m1.weight.data = w.clone()
            conv_count += 1
            continue            
        
        if conv_count % 2 == 1:
            mask = cfg_mask2[layer_id_in_cfg]
            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
            if idx.size == 1:
                idx = np.resize(idx, (1,))
            w = m0.weight.data[idx.tolist(), :, :, :].clone()
            m1.weight.data = w.clone()
            layer_id_in_cfg += 1
            conv_count += 1
            continue

    elif isinstance(m0, nn.BatchNorm2d):
        if conv_count == 2:
            m1.weight.data = m0.weight.data.clone()
            m1.bias.data = m0.bias.data.clone()
            m1.running_mean = m0.running_mean.clone()
            m1.running_var = m0.running_var.clone()            
            continue
        if conv_count % 2 == 1:
            m1.weight.data = m0.weight.data.clone()
            m1.bias.data = m0.bias.data.clone()
            m1.running_mean = m0.running_mean.clone()
            m1.running_var = m0.running_var.clone()
            continue
        if conv_count % 2 == 0:
            mask = cfg_mask2[layer_id_in_cfg-1]
            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
            if idx.size == 1:
                idx = np.resize(idx, (1,))
            m1.weight.data = m0.weight.data[idx.tolist()].clone()
            m1.bias.data = m0.bias.data[idx.tolist()].clone()
            m1.running_mean = m0.running_mean[idx.tolist()].clone()
            m1.running_var = m0.running_var[idx.tolist()].clone()
            continue       

    elif isinstance(m0, nn.Linear):
        mask = cfg_mask2[layer_id_in_cfg-1]
        idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))
        if idx.size == 1:
            idx = np.resize(idx, (1,))
        m1.weight.data = m0.weight.data[:, idx.tolist()].clone()
        m1.bias.data = m0.bias.data.clone()



torch.save({'cfg': cfg, 'state_dict': newmodel.state_dict()}, os.path.join(args.save, 'respruned.pth.tar'))

num_parameters = sum([param.nelement() for param in newmodel.parameters()])
print(newmodel)
model = newmodel
acc = test(model)
acc=acc.numpy()

print("number of parameters: "+str(num_parameters))
with open(os.path.join(args.save, "respruned.txt"), "w") as fp:
    fp.write("Number of parameters: \n"+str(num_parameters)+"\n")
    fp.write("Test accuracy: \n"+str(acc)+"\n")

with torch.cuda.device(0):
  net = model
  flops, params = get_model_complexity_info(net, (3, 32,32), as_strings=True, print_per_layer_stat=True)
  print('Flops:  ' + flops)
  print('Params: ' + params)